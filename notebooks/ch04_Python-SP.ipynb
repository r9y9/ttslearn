{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第4章 Python による音声信号処理\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/r9y9/ttslearn/blob/master/notebooks/ch04_Python-SP.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -VV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ttslearn のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "try:\n",
    "    import ttslearn\n",
    "except ImportError:\n",
    "    !pip install ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttslearn\n",
    "ttslearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ttslearn の動作確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.dnntts import DNNTTS\n",
    "from IPython.display import Audio\n",
    "\n",
    "engine = DNNTTS()\n",
    "wav, sr = engine.tts(\"日本語音声合成のデモです。\")\n",
    "Audio(wav, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveplot(wav.astype(np.float32), sr, ax=ax)\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### パッケージのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シードの固定\n",
    "from ttslearn.util import init_seed\n",
    "init_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 描画周りの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ttslearn.notebook import get_cmap, init_plot_style, savefig\n",
    "cmap = get_cmap()\n",
    "init_plot_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 数値計算のためのPythonライブラリ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy と Torch を用いた配列の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2,2), dtype=np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros(2,2, dtype=torch.float)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray と torch.Tensor のインタフェースの違い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy では配列のサイズを tuple で与えます\n",
    "x = np.zeros((1,2,3), dtype=np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch では配列のサイズを別々の引数で与えられます\n",
    "y = torch.zeros(1, 2, 3, dtype=torch.float32)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray と torch.Tensor の相互変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2,2), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.zeros((2,2), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor から numpy.ndarray への変換\n",
    "type(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy.ndarray から torch.Tensor への変換\n",
    "type(torch.from_numpy(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy.ndarray と torch.Tensor のメモリ共有"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((2,2), dtype=np.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.from_numpy(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0,0] = 1.0 # メモリが共有されているため、 x への変更は y にも反映されます"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 音声ファイルの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.io.wavfile を利用した音声ファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "import ttslearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr, wav = wavfile.read(ttslearn.util.example_audio_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wav) / sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(wav)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 音声の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,2))\n",
    "librosa.display.waveplot(x.astype(np.float32), sr, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-2\n",
    "savefig(\"fig/pyssp_waveplot\")\n",
    "\n",
    "# オーディオプレイヤーの表示\n",
    "Audio(x.astype(np.float32), rate=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 音声のフーリエ変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 振幅スペクトル\n",
    "X = np.abs(np.fft.rfft(x))\n",
    "# 対数振幅スペクトル\n",
    "logX = 20*np.log10(X)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True)\n",
    "freq = np.arange(len(X)) * sr / 2 / len(X)\n",
    "ax[0].plot(freq, X)\n",
    "ax[0].set_title(\"Amplitude spectrum\")\n",
    "ax[0].set_xlim(0, sr // 2)\n",
    "ax[0].set_xlabel(\"Frequency [Hz]\")\n",
    "ax[0].set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax[1].plot(freq, logX)\n",
    "ax[1].set_title(\"Log amplitude spectrum\")\n",
    "ax[1].set_xlabel(\"Frequency [Hz]\")\n",
    "ax[1].set_ylabel(\"Amplitude [dB]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-3\n",
    "savefig(\"fig/pyssp_rfftplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 音声の短時間フーリエ変換とその逆変換"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 窓関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1024\n",
    "n = np.arange(N)\n",
    "w = 0.5 - 0.5 * np.cos(2*np.pi * n / N)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot(w)\n",
    "ax.set_xlim(0, N)\n",
    "ax.set_xlabel(\"Time [sample]\")\n",
    "ax.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 短時間フーリエ変換の実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hanning(N):\n",
    "    n = np.arange(N)\n",
    "    w = 0.5 - 0.5 * np.cos(2*np.pi * n / N)\n",
    "    return w\n",
    "\n",
    "def stft(x, N, S):\n",
    "    # 窓関数（簡単のため、窓幅とフレーム長 N は同じとします）\n",
    "    w = hanning(N)\n",
    "    # 短時間フーリエ変換のフレーム数\n",
    "    M = (len(x) - N) // S + 1\n",
    "    # 短時間フーリエ変換の結果格納用の 2 次元配列\n",
    "    X = np.zeros((M, N//2 + 1), dtype=complex)\n",
    "    # 音声をずらして切り出し、フーリエ変換\n",
    "    for m in range(M):\n",
    "        x_m = w * x[m*S:m*S+N]\n",
    "        X[m, :] = np.fft.rfft(x_m)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 短時間フーリエ変換の結果の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "\n",
    "# 5 ミリ秒のフレームシフトを考えます\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "# スペクトログラム\n",
    "X = stft(x.astype(np.float32), n_fft, frame_shift)\n",
    "# 対数振幅に変換\n",
    "logX = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4), sharex=True)\n",
    "img = librosa.display.specshow(logX.T, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "# 音声のパワーは低域に集中するため、8000 Hz までを表示する\n",
    "ax.set_ylim(0, 8000)\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-5\n",
    "savefig(\"fig/pyssp_stft_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### librosa.stft を用いた短時間フーリエ変換\n",
    "\n",
    "librosa.stftは、STFTを実行する前にデフォルトで信号の冒頭と末尾にパディング処理を行います。前述のSTFT実装はこの処理をサポートしていないため、同等のSTFTの結果を得るためには、center=Falseとしてパディング処理を行わないように設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "# n_fft: 2048, frame_shift: 240\n",
    "X = librosa.stft(x.astype(np.float32), n_fft=n_fft, win_length=n_fft, hop_length=frame_shift, window=\"hann\", center=False).T\n",
    "# 対数振幅に変換\n",
    "logX = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,4), sharex=True)\n",
    "img = librosa.display.specshow(logX.T, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax)\n",
    "fig.colorbar(img, ax=ax, format=\"%+2.f dB\")\n",
    "# 音声のパワーは低域に集中するため、8000 Hz までを表示する\n",
    "ax.set_ylim(0, 8000)\n",
    "\n",
    "ax.set_xlabel(\"Time [sec]\")\n",
    "ax.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 時間解像度と周波数解像度のトレードオフ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_power_of_2(x):\n",
    "    return 1 if x == 0 else 2**(x - 1).bit_length()\n",
    "\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5), sharex=True, sharey=True)\n",
    "\n",
    "for idx, win_length_ms in enumerate([0.05, 0.02, 0.01]):\n",
    "    win_length = int(sr * win_length_ms)\n",
    "    frame_shift = win_length // 4\n",
    "    n_fft = next_power_of_2(win_length)\n",
    "    \n",
    "    X = librosa.stft(x.astype(np.float32), n_fft=n_fft, win_length=n_fft, hop_length=frame_shift).T\n",
    "    logX =  librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "    mesh = librosa.display.specshow(\n",
    "        logX.T, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[idx])\n",
    "    fig.colorbar(mesh, ax=ax[idx], format=\"%+2.f dB\")\n",
    "    ax[idx].set_title(f\"win_length: {win_length}\")\n",
    "    mesh.set_clim(-80, 0)\n",
    "    ax[idx].set_xlim(1.0, 1.5)\n",
    "    ax[idx].set_xticks([1.0, 1.25, 1.5])\n",
    "    # あとでラベルを付け直すので、ここでは消しておく\n",
    "    ax[idx].set_ylabel(\"\")\n",
    "\n",
    "ax[0].set_ylabel(\"Frequency [Hz]\")\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylim(0, 8000)\n",
    "    a.xaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-6\n",
    "savefig(\"fig/pyssp_stft_tradeoff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆短時間フーリエ変換による音声の復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 5 ミリ秒のフレームシフトを考えます\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "\n",
    "# STFT\n",
    "X = librosa.stft(x.astype(np.float32), n_fft=n_fft, win_length=n_fft, hop_length=frame_shift, window=\"hann\")\n",
    "# ISTFT\n",
    "x_hat = librosa.istft(X, win_length=n_fft, hop_length=frame_shift, window=\"hann\")\n",
    "\n",
    "IPython.display.display(Audio(x.astype(np.float32), rate=sr))\n",
    "IPython.display.display(Audio(x_hat.astype(np.float32), rate=sr))\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,4), sharey=True)\n",
    "ax[0].set_title(\"Original speech\")\n",
    "ax[1].set_title(\"Reconstructed speech by ISTFT\")\n",
    "librosa.display.waveplot(x.astype(np.float32), sr, ax=ax[0])\n",
    "librosa.display.waveplot(x_hat.astype(np.float32), sr, ax=ax[1])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 メルスペクトログラム"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メルフィルタバンク"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "n_fft = 2048\n",
    "n_mels = 8\n",
    "\n",
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "x = x.astype(np.float32)\n",
    "x = librosa.resample(x, sr, 16000)\n",
    "sr = 16000\n",
    "\n",
    "# 5 ミリ秒のフレームシフトを考えます\n",
    "frame_shift = int(sr * 0.005)\n",
    "# STFT\n",
    "X = librosa.stft(x, n_fft=n_fft, win_length=n_fft, hop_length=frame_shift, window=\"hann\")\n",
    "# 1 フレームを切り出す\n",
    "X_m = np.abs(X[:, 280])\n",
    "\n",
    "# メルフィルタバンク: n_mels 個のフィルタから構成されます\n",
    "melfb = librosa.filters.mel(sr, n_fft, n_mels=n_mels, norm=None)\n",
    "freq = librosa.fft_frequencies(sr, n_fft)\n",
    "\n",
    "# メルフィルタバンクを表示\n",
    "fig, ax = plt.subplots(n_mels+1, 2, figsize=(8,10), sharex=True)\n",
    "ax[0][0].plot(freq, np.ones_like(freq))\n",
    "ax[0][0].set_title(\"All pass filter\")\n",
    "ax[0][0].set_ylim(0,1.1)\n",
    "ax[0][1].plot(freq, X_m)\n",
    "ax[0][1].set_title(\"Input amplitude spectrum\")\n",
    "for idx, fb in enumerate(melfb):\n",
    "    ax[idx+1][0].plot(freq, fb)\n",
    "    ax[idx+1][0].set_title(f\"Filter {idx+1}\")\n",
    "    ax[idx+1][1].plot(freq, fb * X_m)\n",
    "    ax[idx+1][1].set_title(f\"Filtered amplitude {idx+1}\")\n",
    "\n",
    "for a,b in ax:\n",
    "    a.set_xlabel(\"Frequency [Hz]\")\n",
    "    b.set_xlabel(\"Frequency [Hz]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-7\n",
    "savefig(\"fig/pyssp_melfb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メルスペクトログラムの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 5 ミリ秒のフレームシフトを考えます\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "\n",
    "# スペクトログラム\n",
    "X = librosa.stft(x.astype(np.float32), n_fft=n_fft, hop_length=frame_shift)\n",
    "\n",
    "# 80 次元のメルスペクトログラム\n",
    "n_mels = 80\n",
    "melfb = librosa.filters.mel(sr, n_fft, n_mels=n_mels)\n",
    "melspec = librosa.amplitude_to_db(np.dot(melfb, np.abs(X)), ref=np.max)\n",
    "\n",
    "# 比較用の対数振幅スペクトログラム\n",
    "logX = librosa.amplitude_to_db(np.abs(X), ref=np.max)\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "ax[0].set_title(\"Spectrogram\")\n",
    "ax[1].set_title(\"80-dim Mel-spectrogram\")\n",
    "mesh = librosa.display.specshow(logX, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"hz\", ax=ax[0])\n",
    "fig.colorbar(mesh, ax=ax[0], format=\"%+2.f dB\")\n",
    "mesh.set_clim(-80, 0)\n",
    "mesh = librosa.display.specshow(melspec, hop_length=frame_shift, sr=sr, cmap=cmap, x_axis=\"time\", y_axis=\"mel\",ax=ax[1])\n",
    "fig.colorbar(mesh, ax=ax[1], format=\"%+2.f dB\")\n",
    "mesh.set_clim(-80, 0)\n",
    "\n",
    "for a in ax:\n",
    "    a.set_ylim(0, 8000)\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Frequency [Hz]\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-8\n",
    "savefig(\"fig/pyssp_melspectrogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Griffin-Lim のアルゴリズムに基づく位相復元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 音声ファイルの読み込み\n",
    "sr, x = wavfile.read(ttslearn.util.example_audio_file())\n",
    "# 5 ミリ秒のフレームシフトを考えます\n",
    "frame_shift = int(sr * 0.005)\n",
    "n_fft = 2048\n",
    "\n",
    "# 振幅スペクトログラム\n",
    "X = np.abs(librosa.stft(x.astype(np.float32), n_fft=n_fft, hop_length=frame_shift))\n",
    "\n",
    "y1 = librosa.griffinlim(X, hop_length=frame_shift, n_iter=1)\n",
    "y2 = librosa.griffinlim(X, hop_length=frame_shift, n_iter=100)\n",
    "\n",
    "# オーディオプレイヤーの表示\n",
    "IPython.display.display(Audio(y1, rate=sr))\n",
    "IPython.display.display(Audio(y2, rate=sr))\n",
    "IPython.display.display(Audio(x, rate=sr))\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(8,6), sharey=True)\n",
    "ax[0].set_title(\"Griffin-Lim # of iteration: 1\")\n",
    "ax[1].set_title(\"Griffin-Lim # of iteration: 100\")\n",
    "ax[2].set_title(\"Natural speech\")\n",
    "librosa.display.waveplot(y1, sr=sr, ax=ax[0])\n",
    "librosa.display.waveplot(y2, sr=sr, ax=ax[1])\n",
    "librosa.display.waveplot(x.astype(np.float32), sr=sr, ax=ax[2])\n",
    "\n",
    "for a in ax:\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_ylabel(\"Amplitude\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# 図4-9\n",
    "savefig(\"fig/pyssp_griffin_lim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 瞬時周波数の可視化 (bonus)\n",
    "\n",
    "Griffin-Limのアルゴリズムは、位相復元手法です。合成音声と自然音声の瞬時位相（位相の時間微分）を比較することで、位相復元が期待通り行われているかを視覚的に確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_fft = 1024\n",
    "hop_length = n_fft // 4\n",
    "fig, ax = plt.subplots(1, 3, figsize=(10,5), sharex=True)\n",
    "\n",
    "C = librosa.stft(y1, n_fft=n_fft, hop_length=hop_length)\n",
    "ifreq = np.angle(C[:, 1:] * np.conjugate(C[:, :-1]))\n",
    "mesh = librosa.display.specshow(ifreq, cmap=cmap, ax=ax[0], x_axis=\"time\", y_axis=\"hz\", sr=sr, hop_length=hop_length)\n",
    "fig.colorbar(mesh, ax=ax[0])\n",
    "ax[0].set_title(\"GL # of iteration: 1\")\n",
    "\n",
    "C = librosa.stft(y2, n_fft=n_fft, hop_length=hop_length)\n",
    "ifreq = np.angle(C[:, 1:] * np.conjugate(C[:, :-1]))\n",
    "mesh = librosa.display.specshow(ifreq, cmap=cmap, ax=ax[1], x_axis=\"time\", y_axis=\"hz\", sr=sr, hop_length=hop_length)\n",
    "fig.colorbar(mesh, ax=ax[1])\n",
    "ax[1].set_title(\"GL # of iteration: 100\")\n",
    "\n",
    "C = librosa.stft(x.astype(np.float32), n_fft=n_fft, hop_length=hop_length)\n",
    "ifreq = np.angle(C[:, 1:] * np.conjugate(C[:, :-1]))\n",
    "mesh = librosa.display.specshow(ifreq, cmap=cmap, ax=ax[2], x_axis=\"time\", y_axis=\"hz\", sr=sr, hop_length=hop_length)\n",
    "fig.colorbar(mesh, ax=ax[2])\n",
    "ax[2].set_title(\"Natural speech\")\n",
    "\n",
    "for a in ax:\n",
    "    # あとでラベルを付け直すので、ここでは消しておく\n",
    "    a.set_ylabel(\"\")\n",
    "\n",
    "ax[0].set_ylabel(\"Frequency [Hz]\")\n",
    "for a in ax:\n",
    "    a.set_xlim(1.5, 3.0)\n",
    "    a.set_ylim(0, 4000)\n",
    "    a.set_xlabel(\"Time [sec]\")\n",
    "    a.set_xticks([1.5, 2.0, 2.5, 3.0])\n",
    "    \n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
